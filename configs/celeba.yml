data:
    dataset: "CelebA_HQ"
    image_size: 256
    channels: 3
    
model:
    type: "simple"
    in_channels: 3
    out_ch: 3
    ch: 128
    ch_mult: [1, 1, 2, 2, 4, 4]
    num_res_blocks: 2
    attn_resolutions: [16, ]
    dropout: 0.0
    var_type: fixedsmall
    ema_rate: 0.999
    ema: True
    resamp_with_conv: True

diffusion:
    beta_schedule: linear
    num_timesteps: 1000
    beta_start: 0.0001
    beta_end: 0.02

samplingDDIM:
    beta_schedule: linear
    beta_start: 0.0001
    beta_end: 0.02
    DDPM_num_timesteps: 1000 
    sigma : 0.00   # generate random source samples with sigma 0.008 0.006 and 0.005
    t : 700 ###  The maximum time steps for inversion and denoising in DDIM
    stepsize_forward: 10 
    stepsize_backward: 50  


training:
    random_source_img: "./data/training/source"
    target_img: "./data/training/target"
    output_img: "./data/training/output"
    n_epochs: 3
    batch_size : 6
    lr : 0.000004
    clip_classifier_text_source : "face "
    clip_classifier_text_target : "Fear face"
    emonet_emotion: 4 #emotion_classes = {0:"Neutral", 1:"Happy", 2:"Sad", 3:"Surprise", 4:"Fear", 5:"Disgust"}                
    use_finetuned_classifier: "off"  # or on
    ref_img: "./data/training/ref_img.jpg"



checkpoints:
    pretrained_diffusion_checkpoint: "./pre_trained/celeba_hq.ckpt"
    pretrained_classifier_FACEID: ./pre_trained/model_ir_se50.pth
    pretrained_classifier_emonet: ./pre_trained/emonet_5.pth
    finetuned_diffusion_checkpoints: ./data/checkpoints   # where to save the fine-tuned weights


classifier: clip  # clip / emonet
